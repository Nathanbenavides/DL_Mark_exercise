{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fuddkGizGtK"
   },
   "source": [
    "# Marked exercises after Lecture 5\n",
    "This notebook contains the marked exercises with instructions and explanations.\n",
    "\n",
    "Work through the cells below in sequential order, executing each cell as you progress. Throughout the notebook, you will encounter instructions marked with the words **YOUR CODE HERE** followed by **raise NotImplementedError()**. You will have to substitute  *raise NotImplementedError()* with your own code.\n",
    "Follow the instructions and write the code to complete the tasks.\n",
    "\n",
    "Along the way, you may also find questions. Try to reflect on the questions before/after running the code.\n",
    "\n",
    "You will have to implement a MultiHeadAttention.\n",
    "\n",
    "You have 2 exercises to complete. In total, you can get **20 points** out of 60 points for Submission 1 for completing all marked exercises related to lecture 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJWhgH8WC6tt"
   },
   "source": [
    "This notebook was developed at the [Idiap Research Institute](https://www.idiap.ch) by [Alina Elena Baia](mailto:alina.baia.idiap.ch>), [Darya Baranouskaya](mailto:darya.baranouskaya.idiap.ch) and [Olena Hrynenko](mailto:olena.hrynenko.idiap.ch) (equal contribution).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS0TBNueVYUS"
   },
   "source": [
    "Read the paper ['Attention is all you need'](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "and implement Scaled dot-product and Multi-head attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yaluUJuBCss"
   },
   "source": [
    "You are NOT ALLOWED to use toolboxes that automatically solve the main tasks of the assignment, such as (but not limited to) nn.MultiheadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T08:51:06.824975Z",
     "iopub.status.busy": "2024-03-20T08:51:06.822241Z",
     "iopub.status.idle": "2024-03-20T08:51:29.120803Z",
     "shell.execute_reply": "2024-03-20T08:51:29.118299Z",
     "shell.execute_reply.started": "2024-03-20T08:51:06.824896Z"
    },
    "id": "Oh2sgIwBrv4r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# you are not allowed to use any other libraries (like numpy )in this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5n3ksk97hVrB"
   },
   "source": [
    "##### 1.5.1 Scaled dot-product attention\n",
    "\n",
    "'An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.'\n",
    "\n",
    "\n",
    "\n",
    "The input of the non-batched scaled dot-product attention is 3 matrixes: queries $Q\\in\\mathbb{R}^{L\\times d_k}$, keys $K\\in\\mathbb{R}^{S\\times d_k}$  and values  $V\\in\\mathbb{R}^{S\\times d_v}$, where $L$ and $S$ represent sequences length (for example, number of tokens in the query and key sequences), and $d_k,d_k, d_v$ are the dimensions of query, key, value correspondingly.\n",
    "\n",
    "So, query is a sequence of $L$ token embeddings, each token of dimension $d_k$, key is a sequence of $S$ token embeddings, each token of dimension $d_k$, and value is a sequence of $S$ token embeddings, each token of dimension $d_v$.\n",
    "\n",
    "Scaled dot-product attention is computed as:\n",
    "$$ \\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "Note that your implementation should work for batched inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T08:51:29.125772Z",
     "iopub.status.busy": "2024-03-20T08:51:29.124856Z",
     "iopub.status.idle": "2024-03-20T08:51:29.140022Z",
     "shell.execute_reply": "2024-03-20T08:51:29.137589Z",
     "shell.execute_reply.started": "2024-03-20T08:51:29.125704Z"
    },
    "id": "eM1zjNlNgWF2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f91f03f00041eb4b0c11c1c3feb87493",
     "grade": false,
     "grade_id": "cell-31abf445a7e9fa51",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Shadow\\AppData\\Local\\Temp\\ipykernel_7964\\3509506775.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product(query, key, value):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query: torch.Tensor (..., L, d_k)\n",
    "        key: torch.Tensor (..., S, d_k)\n",
    "        value: torch.Tensor (..., S, d_v)\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        attn: torch.Tensor (..., L, d_v), output of the scaled dot-product attention (\\softmax(Q K^T / d) V\n",
    "        attn_weights: torch.Tensor (..., L, d_v), attention weights (\\softmax(Q K^T / d))\n",
    "\n",
    "    L is the length of query sequence\n",
    "    S is the length of key and value sequences, d_k and d_v are the embeddings dimensions\n",
    "\n",
    "    #... is a placeholder to denote other dimensions.\n",
    "     The scaled_dot_product should be computed on the last and second-to-last dimension.\n",
    "     Every element in ...dimension should be processed independently (torch matmul operations allow  that to happen).\n",
    "     For example, ... can represent a batch size B and the vector query will have a size (B, L, d_k),\n",
    "     then the output should be a size of (B, L, d_v) where every batch is processed independently from other batches\n",
    "    \"\"\"\n",
    "    #TODO implement scaled dot product\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    d_k = query.size(-1)\n",
    "    attn_weights = torch.nn.functional.softmax(torch.matmul(query, torch.transpose(key, -2, -1)) / (d_k ** 0.5), dim=-1)\n",
    "    attn = torch.matmul(attn_weights, value)\n",
    "\n",
    "    return attn, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T08:51:29.144285Z",
     "iopub.status.busy": "2024-03-20T08:51:29.143529Z",
     "iopub.status.idle": "2024-03-20T08:51:29.342978Z",
     "shell.execute_reply": "2024-03-20T08:51:29.340326Z",
     "shell.execute_reply.started": "2024-03-20T08:51:29.144219Z"
    },
    "id": "hui0ce-jvjHE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check your implementation\n",
    "#epsilon to check your results\n",
    "epsilon = 1e-3\n",
    "epsilon_2 = 1e-6\n",
    "#example 1\n",
    "query  = torch.Tensor([[1, 2, 3],\n",
    "                       [3, 2, 1]])\n",
    "key  = torch.Tensor([[3, 2, 1],\n",
    "                     [1, 2, 3],\n",
    "                     [1, 2, 3]])\n",
    "value  = torch.Tensor([[1, 1],\n",
    "                       [1, 1],\n",
    "                       [1, 1]])\n",
    "\n",
    "answer, attn_weights = scaled_dot_product(query, key, value)\n",
    "answer\n",
    "correct = torch.Tensor([[1.0000, 1.0000],\n",
    "        [1.0000, 1.0000]])\n",
    "\n",
    "assert (torch.all(answer + epsilon >= correct)) and (torch.all(answer - epsilon <= correct))\n",
    "assert (torch.all(attn_weights.sum(dim=1) + epsilon_2 >= torch.ones(2))) and (torch.all(attn_weights.sum(dim=1) - epsilon_2 <= torch.ones(2)))\n",
    "\n",
    "#example 2\n",
    "#change values and see the result\n",
    "value  = torch.Tensor([[1, 2],\n",
    "                       [2, 1],\n",
    "                       [1, 1]])\n",
    "\n",
    "answer, attn_weights = scaled_dot_product(query, key, value)\n",
    "\n",
    "correct = torch.Tensor([[1.4763, 1.0473],\n",
    "        [1.0829, 1.8343]])\n",
    "assert (torch.all(answer + epsilon >= correct)) and (torch.all(answer - epsilon <= correct))\n",
    "assert (torch.all(attn_weights.sum(dim=1) + epsilon_2 >= torch.ones(2))) and (torch.all(attn_weights.sum(dim=1) - epsilon_2 <= torch.ones(2)))\n",
    "\n",
    "\n",
    "#example 3\n",
    "#change values and see the result\n",
    "value  = torch.Tensor([[1, 2],\n",
    "                       [1, 1],\n",
    "                       [1, 2]])\n",
    "\n",
    "answer, attn_weights = scaled_dot_product(query, key, value)\n",
    "answer\n",
    "correct = torch.Tensor([[1.0000, 1.5237],\n",
    "        [1.0000, 1.9171]])\n",
    "assert (torch.all(answer + epsilon >= correct)) and (torch.all(answer - epsilon <= correct))\n",
    "assert (torch.all(attn_weights.sum(dim=1) + epsilon_2 >= torch.ones(2))) and (torch.all(attn_weights.sum(dim=1) - epsilon_2 <= torch.ones(2)))\n",
    "\n",
    "\n",
    "#example 4\n",
    "query  = torch.Tensor([[1, 2, 3],\n",
    "                       [3, 2, 1]])\n",
    "key  = torch.Tensor([[3, 2, 1],\n",
    "                     [1, 2, 3]])\n",
    "value  = torch.Tensor([[1, 1],\n",
    "                       [1, 1]])\n",
    "\n",
    "answer, attn_weights = scaled_dot_product(query, key, value)\n",
    "answer\n",
    "correct = torch.Tensor([[1.0000, 1.0000],\n",
    "        [1.0000, 1.0000]])\n",
    "\n",
    "assert (torch.all(answer + epsilon >= correct)) and (torch.all(answer - epsilon <= correct))\n",
    "assert (torch.all(attn_weights.sum(dim=1) + epsilon_2 >= torch.ones(2))) and (torch.all(attn_weights.sum(dim=1) - epsilon_2 <= torch.ones(2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hBQMEJIgja2"
   },
   "source": [
    "##### 1.5.1 Multi-Head attention\n",
    "\n",
    "For the multi-head attention the dimension of queries, keys and values are equal to $d\\_model$.\n",
    "\n",
    "'Instead of performing a single attention function with d_model-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d\\_k$, $d\\_k$ and $d\\_v$ dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$ -dimensional output values. These are concatenated and once again projected, resulting in the final values.\n",
    "\n",
    "$$\n",
    "\\begin{split}\\begin{split}\n",
    "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
    "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
    "\\end{split}\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "Where the projections are parameter matrices $W_i^Q \\in R^{d\\_model ×d\\_k}$, $W_i^K \\in R^{d\\_model ×d\\_k}$, $W_i^V \\in R^{d\\_model ×d\\_v}$ and $W^O \\in R^{h*d\\_v ×d\\_model}$\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T08:51:29.347353Z",
     "iopub.status.busy": "2024-03-20T08:51:29.346757Z",
     "iopub.status.idle": "2024-03-20T08:51:29.368192Z",
     "shell.execute_reply": "2024-03-20T08:51:29.366608Z",
     "shell.execute_reply.started": "2024-03-20T08:51:29.347300Z"
    },
    "id": "dkuRbiJLzJoc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd9e624da946f04ea8657e5dd2aec227",
     "grade": false,
     "grade_id": "cell-310772a2933a29fa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, h, d_model, d_k, d_v):\n",
    "        '''\n",
    "        d_model: dimensionality of embeddings (total)\n",
    "        h: number of heads\n",
    "        d_k: dimensionality of one linear projections of query\n",
    "        d_v: dimensionality of one linear projections on value\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        assert d_model // d_v == h #we want the output to have the same dimensionality d_model as the inputs\n",
    "\n",
    "        # Note: no bias is needed when linear projections are performed\n",
    "        self.num_heads = h\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, h * d_k, bias=False) # - linear projection of q\n",
    "        self.k_proj = nn.Linear(d_model, h * d_k, bias=False) # - linear projection of k\n",
    "        self.v_proj = nn.Linear(d_model, h * d_v, bias=False) # - linear projection of v\n",
    "        self.o_proj = nn.Linear(h * d_v, d_model, bias=False) # - linear projection after concatenation\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        '''\n",
    "        Args:\n",
    "            query: torch.Tensor (Batch_size, L, d_model)\n",
    "            key: torch.Tensor (Batch_size, S, d_model)\n",
    "            value: torch.Tensor (Batch_size, S, d_model)\n",
    "        Returns:\n",
    "            attn: torch.Tensor (Batch_size, L, d_model), output of the multi-head attention\n",
    "            attn_weights: torch.Tensor (Batch_size, h, L, S), attention weights per head\n",
    "        '''\n",
    "\n",
    "        # you are allowed to use previously implemented scaled_dot_product\n",
    "\n",
    "\n",
    "        b, L, S = q.shape[0], q.shape[1], k.shape[1]\n",
    "\n",
    "        # you are not allowed to use a for loop to iterate through different heads\n",
    "        # instead you should:\n",
    "        #   1) get linear projections for q, k, and v (for example, get q_proj of size (b, L, h * d_k) from q)\n",
    "        #   2) reshape the projections: split the channel dimension (h * d_k (or h * d_v)) into 2 dimensions (h and d_k (or h and d_v))\n",
    "        # and then transpose reshaped q_proj_reshaped, k_proj_reshaped and v_proj_reshaped vectors to prepare the input for the scaled dot product.\n",
    "        # The sizes of the reshaped and transposed vectors should be (..., L, d_k), (..., S, d_k), (..., S, d_v), respectively.\n",
    "        # For example, q_proj of size (b, L, h * d_k) should become a vector q_proj_reshaped_transposed of the size (b, h, L, d_k).\n",
    "        #   3) compute the scaled dot product, using reshaped transposed vectors q_proj_reshaped_transposed, \n",
    "        # k_proj_reshaped_transposed, v_proj_reshaped_transposed as inputs\n",
    "        #   4) transpose and reshape the output of the scaled dot-product attention \n",
    "        # (concatenate head outputs to get a h*d_v dimension) to get an output of size (b, L, d_model)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        q_proj = self.q_proj(q)\n",
    "        k_proj = self.k_proj(k)\n",
    "        v_proj = self.v_proj(v)\n",
    "\n",
    "        q_proj_reshaped = q_proj.view(b, L, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k_proj_reshaped = k_proj.view(b, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v_proj_reshaped = v_proj.view(b, S, self.num_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        attn, attn_weights = scaled_dot_product(q_proj_reshaped, k_proj_reshaped, v_proj_reshaped)\n",
    "        attn = self.o_proj(attn.transpose(1, 2).contiguous().view(b, L, self.num_heads * self.d_v))\n",
    "\n",
    "        return attn, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-03-20T08:52:10.386661Z",
     "iopub.status.busy": "2024-03-20T08:52:10.370556Z",
     "iopub.status.idle": "2024-03-20T08:52:10.494414Z",
     "shell.execute_reply": "2024-03-20T08:52:10.423600Z",
     "shell.execute_reply.started": "2024-03-20T08:52:10.386417Z"
    },
    "id": "Ny9uFgj7CPtN",
    "outputId": "0e54398d-dc19-4c14-97ad-5a2e4b936625",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10, 512]), torch.Size([16, 8, 10, 15]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 16 #batch size\n",
    "h = 8 # number of heads\n",
    "L = 10 #length of the query sequence\n",
    "S = 15 #length of the key and value sequences\n",
    "d_model = 512\n",
    "d_k = 50\n",
    "d_v = 64\n",
    "\n",
    "multihead_attn = MultiHeadAttention(h, d_model, d_k, d_v)\n",
    "\n",
    "q, k, v = torch.rand((b, L, d_model)), torch.rand((b, S, d_model)), torch.rand((b, S, d_model))\n",
    "\n",
    "attn, attn_weights = multihead_attn(q, k, v)\n",
    "\n",
    "assert list(attn_weights.shape) == [b, h, L, S]\n",
    "assert list(attn.shape) == [b, L, d_model]\n",
    "assert torch.all(attn_weights.sum(dim=-1) + 1e-3 >= torch.ones((b, h, L))) and torch.all(attn_weights.sum(dim=-1) - 1e-3 <= torch.ones((b, h, L)))\n",
    "attn.shape, attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2024-03-20T09:00:47.821727Z",
     "iopub.status.busy": "2024-03-20T09:00:47.820046Z",
     "iopub.status.idle": "2024-03-20T09:00:49.403472Z",
     "shell.execute_reply": "2024-03-20T09:00:49.400827Z",
     "shell.execute_reply.started": "2024-03-20T09:00:47.821633Z"
    },
    "id": "vAeUDI16TGSB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "681db83e906c7d86f76fdaaf910f28aa",
     "grade": false,
     "grade_id": "cell-cf90187a36d56c44",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "fbb1e2e5-cb3e-444d-b0be-f2b24cd5704e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 512]) torch.Size([16, 8, 10, 15])\n"
     ]
    }
   ],
   "source": [
    "#check the documentation of pytorch MultiheadAttention: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "\n",
    "# which parameters should you give to torch  multi-head attention\n",
    "#  so that the output will be in the same format as you implemented above?\n",
    "# the inner dimensionality of q, k, v projections can be different from your implementation, as torch implementat differs\n",
    "\n",
    "multihead_attn_torch = nn.MultiheadAttention(\n",
    "                                             \n",
    "                                            #add other parameters if needed\n",
    "                                            # YOUR CODE HERE\n",
    "                                            # raise NotImplementedError()\n",
    "                                             embed_dim=d_model,\n",
    "                                             num_heads=h,\n",
    "                                             kdim=d_model,\n",
    "                                             vdim=d_model,\n",
    "                                             batch_first=True,\n",
    "                                             \n",
    "                                            )\n",
    "\n",
    "attn_output_torch, attn_output_weights_torch = multihead_attn_torch(q, k, v,\n",
    "                                                                  # add other parameters if needed\n",
    "                                                                  # YOUR CODE HERE\n",
    "                                                                  # raise NotImplementedError()\n",
    "                                                                    need_weights=True,\n",
    "                                                                    average_attn_weights=False\n",
    "                                                                   )\n",
    "print(attn_output_torch.shape, attn_output_weights_torch.shape)\n",
    "\n",
    "assert attn.shape == attn_output_torch.shape\n",
    "assert attn_weights.shape == attn_output_weights_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
